{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Machine Learning Classifier for Sorting Lego Pieces  \n",
    "## Description:  \n",
    "Developing and training algorithms for image-based classification of four different Lego categories in a conveyor system.\n",
    "\n",
    "## Group#: 8  \n",
    "\n",
    "## Authors:  \n",
    "- Amer Alhamwi: 15976814  \n",
    "- Wael Hamid: 80105687 \n",
    "\n",
    "University of British Columbia Okanagan (UBCO)  \n",
    "\n",
    "## Date:  \n",
    "December 5, 2024  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries essential for image processing, \n",
    "# data analysis, and machine learning.\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from PIL import Image, ImageFilter\n",
    "from skimage import exposure\n",
    "from scipy.ndimage import gaussian_filter,binary_dilation, binary_erosion\n",
    "from numpy import asarray\n",
    "from skimage.filters import threshold_otsu\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "from skimage.measure import find_contours\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage 1 code starts here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code loads images from the specified folder, applies image cropping \n",
    "# to enhance training accuracy, and assigns classification labels based on the first \n",
    "# letter of each image's name. The processed images and labels are stored in arrays \n",
    "# for subsequent use in the training process.\n",
    "\n",
    "\n",
    "# Declare the two folders names for image files\n",
    "path = \"c:/Users/Admin/Desktop/ENGR 418/Assignments/Project/Project_Stage_2/\"\n",
    "\n",
    "def test_function(path):\n",
    "    folder_training = os.path.join(path, \"training\")\n",
    "    folder_testing = os.path.join(path, \"testing\")\n",
    "    \n",
    "    return folder_training, folder_testing\n",
    "\n",
    "folder_training, folder_testing = test_function(path)\n",
    "\n",
    "# Define the function before calling it\n",
    "def get_image_data(folder):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for pic in os.listdir(f\"{folder}/\"):\n",
    "        # opening image using image from PIL\n",
    "        image = Image.open(f\"{folder}/{pic}\").convert(\"L\")\n",
    "        # Crop the images to the center to make it more accurate\n",
    "        top = int(np.floor(image.height / 4))\n",
    "        bottom = int(np.ceil(image.height * (3 / 4)))\n",
    "        left = int(np.floor(image.width / 4))\n",
    "        right = int(np.ceil(image.width * (3 / 4)))\n",
    "        #print(image.size)\n",
    "\n",
    "        # new image is about [500*500] pixels\n",
    "        cropped_image = image.crop((left, top, right, bottom))\n",
    "\n",
    "        # resizing images to 64*64 pixels (4096)\n",
    "        resized_image = cropped_image.resize((64, 64))\n",
    "        \n",
    "        data = np.asarray(resized_image)  # Convert image to an array\n",
    "        \n",
    "        vec = np.hstack(data)  # Flatten 2D image to a 1D array\n",
    "        \n",
    "        x.append(vec)  # Append the flattened image data to the list\n",
    "\n",
    "        # Assign classes based on first letter/number of the image name\n",
    "        if str.lower(pic[0]) == \"c\":\n",
    "            y.append(0)  # Cir\n",
    "        elif str.lower(pic[0]) == \"r\":\n",
    "            y.append(1)  # rec\n",
    "        elif str.lower(pic[0]) == \"2\":\n",
    "            y.append(2)  # 2b1\n",
    "        else:\n",
    "            y.append(3)  # Squ\n",
    "\n",
    "    x = np.array(x)  # Convert to numpy array\n",
    "    y = np.array(y)  # Convert to numpy array\n",
    "\n",
    "    #print(x)\n",
    "    #print(y)\n",
    "    return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[27  0  0  0]\n",
      " [ 0 27  0  0]\n",
      " [ 0  0 27  0]\n",
      " [ 0  0  0 27]]\n"
     ]
    }
   ],
   "source": [
    "# This cell loads the training data, trains a Logistic \n",
    "# Regression model with the training set, and then\n",
    "# evaluates the model by predicting on the same training data.\n",
    "\n",
    "#Set paths for training and testing folders based on the project path\n",
    "folder_training, folder_testing = test_function(path)\n",
    "\n",
    "x1_train, y1_train = get_image_data(folder_training)\n",
    "\n",
    "# Train the Logistic Regression model \n",
    "model = LogisticRegression(solver='lbfgs', max_iter=1000)  # Increase max_iter to 1000 or more\n",
    "\n",
    "#print(f\"Data from first directory: {x1_train.shape}, Labels: {y1_train.shape}\")\n",
    "\n",
    "# Train the Logistic Regression model \n",
    "model = LogisticRegression(solver='lbfgs', max_iter=1000)  # Increase max_iter to 1000 \n",
    "model.fit(x1_train, y1_train)\n",
    "y_pred1 = model.predict(x1_train)\n",
    "\n",
    "print(\"Accuracy:\", np.round(accuracy_score(y1_train, y_pred1),3))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y1_train, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.454\n",
      "Confusion Matrix:\n",
      " [[15  3  5  4]\n",
      " [ 3 16  1  7]\n",
      " [ 9  4  8  6]\n",
      " [10  6  1 10]]\n"
     ]
    }
   ],
   "source": [
    "# This code block loads the testing data, uses the trained Logistic Regression model \n",
    "# to make predictions on the test set, and evaluates the model's performance by \n",
    "# displaying the accuracy score and a confusion matrix.\n",
    "\n",
    "# Load the testing data\n",
    "x1_test, y1_test = get_image_data(folder_testing)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred2 = model.predict(x1_test)\n",
    "\n",
    "# Print the accuracy and confusion matrix\n",
    "print(\"Accuracy:\", np.round(accuracy_score(y1_test, y_pred2), 3))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y1_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage 2 code starts here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell sets the project folder path and includes two function definitions:  \n",
    "# test_function identifies the training and testing directories, and  \n",
    "# get_image_data processes images into flattened arrays along with their labels.\n",
    "\n",
    "#test_function finds the training and testing folders \n",
    "def test_function(path):\n",
    "    folder_training = os.path.join(path, \"training/\")\n",
    "    folder_testing = os.path.join(path, \"testing/\")\n",
    "    \n",
    "    return folder_training, folder_testing\n",
    "\n",
    "folder_training, folder_testing = test_function(path)\n",
    "\n",
    "#get_image_data returns arrays to store the image horizontallty and their labels \n",
    "def get_image_data(folder, im_width, im_length):\n",
    "    \n",
    "    file_names = os.listdir(folder)\n",
    "    n_samples = len(file_names)\n",
    "    \n",
    "    #Decalre arrays to store the image horizontallty and their labels \n",
    "    x = np.empty((n_samples, im_width * im_length))\n",
    "    y = np.empty((n_samples, 1), dtype=int)\n",
    "    #declaring incrementer to go throght the files \n",
    "    i=0\n",
    "\n",
    "    for pic in os.listdir(f\"{folder}/\"):\n",
    "        \n",
    "        #opening image using image from PIL\n",
    "        image = Image.open(f\"{folder}/{pic}\").convert(\"L\")\n",
    "        #resizing image\n",
    "        resized_image = image.resize((im_width, im_length))\n",
    "        \n",
    "        #Convert image to array and store it hoirizonatallty(row)\n",
    "        image_array = asarray(resized_image)\n",
    "        x[i, :] = image_array.reshape(1, -1)\n",
    "        \n",
    "        #Assign classes based on first letter/number of the image file name\n",
    "        if str.lower(pic[0]) == \"c\":\n",
    "            y[i] = 0  # Cir\n",
    "        elif str.lower(pic[0]) == \"r\":\n",
    "            y[i] = 1  # Rec\n",
    "        elif str.lower(pic[0]) == \"2\":\n",
    "            y[i] = 2  # 2b1\n",
    "        else:\n",
    "            y[i] = 3  # Squ\n",
    "        i+=1        \n",
    "    return x, y.ravel()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell processes images using edge detection techniques to extract features.  \n",
    "# It includes edge filtering, thresholding, and line detection to calculate key metrics.  \n",
    "# The process_image function returns a feature vector to train the model effectively.\n",
    "\n",
    "#process_image returns the features that will be passed to train the model \n",
    "def process_image(image_array,y_train,ET):\n",
    "\n",
    "    #decalring image dimensions \n",
    "    im_width=64\n",
    "    im_length=64\n",
    "\n",
    "    #horizontal/vertical lines & their count values\n",
    "    horizontal_line = np.zeros(im_width)\n",
    "    vertial_line = np.zeros(im_length)\n",
    "    h_count=0\n",
    "    v_count=0\n",
    "    \n",
    "    #degree incrementer\n",
    "    deg_inc=0\n",
    "    # Edge density accumulator\n",
    "    edge_density = 0\n",
    "\n",
    "    #First,start by applying image proccessing and edge filtering... \n",
    "    global_mean = np.mean(image_array)\n",
    "    image_array = image_array - global_mean  # Subtract the global mean\n",
    "    # increase contrast by making image occupy the whole grayscale 0 to 255\n",
    "    image_array = (image_array-np.min(image_array))*255/(np.max(image_array)-np.min(image_array))\n",
    "    # reshpae image to 64x64 and convert to grayscale\n",
    "    im = Image.fromarray(image_array.reshape(im_width,im_length)).convert('L') \n",
    "    # detect the edges and keep them\n",
    "    edges_image = im.filter(ImageFilter.FIND_EDGES) \n",
    "    #convert the edges to array type \n",
    "    edges_array = np.asarray(edges_image).astype(float) \n",
    "    # edge filter produces artificial edges at the boundary, we remove them next\n",
    "    edges_array_scaled = edges_array.copy()[1:im_width-3,1:im_length-3]\n",
    "    # apply edge thresholding and keep only prominent ones\n",
    "    edges_array_scaled[edges_array_scaled < ET,] = 0\n",
    "    #create an image from our final array\n",
    "    im_rotate = Image.fromarray(edges_array_scaled)      \n",
    "\n",
    "    # Next, find horizontal lines and vertical lines in the image \n",
    "\n",
    "    #the goal is keep the biggest value of horzintal and vertical count\n",
    "    while(deg_inc<=360):\n",
    "        # Rotate the PIL Image\n",
    "        rotated_image = im_rotate.rotate(deg_inc)\n",
    "        # Convert rotated image back to a NumPy array\n",
    "        rotated_array = np.asarray(rotated_image)\n",
    "        for y in range(im_width-4):\n",
    "            horizontal_line[y] = np.count_nonzero(rotated_array[y, :]) \n",
    "            if y > 0 and horizontal_line[y] >= horizontal_line[y-1]:\n",
    "                h_count = horizontal_line[y]  # Save the biggest horizontal count\n",
    "\n",
    "        for x in range(im_length-4):\n",
    "            vertial_line[x] = np.count_nonzero(rotated_array[:,x])                         \n",
    "            if x > 0 and vertial_line[x] > vertial_line[x-1]:\n",
    "                v_count = vertial_line[x]     # Save the biggest vertical count\n",
    "\n",
    "  \n",
    "        # Compute edge density for the current rotation\n",
    "        edge_density += (np.count_nonzero(rotated_array) / (rotated_array.size))\n",
    "        deg_inc= deg_inc + 1   \n",
    "\n",
    "    # plt.imshow(rotated_image)\n",
    "    # plt.show()\n",
    "\n",
    "    # Compute sum of vertical and horizontal pixel intensities\n",
    "    edges_v = np.sum(rotated_image,axis=0)\n",
    "    edges_h = np.sum(rotated_image,axis=1)\n",
    "\n",
    "    # Compute 45-degree diagonal edges pixel intensities\n",
    "    edges_diag_45 = [np.sum(np.diag(np.fliplr(edges_array_scaled), k)) for k in range(-edges_array_scaled.shape[0] + 1, edges_array_scaled.shape[1])]\n",
    "    # Compute -45-degree diagonal edges  pixel intensities\n",
    "    edges_diag_neg45 = [np.sum(np.diag(edges_array_scaled, k)) for k in range(-edges_array_scaled.shape[0] + 1, edges_array_scaled.shape[1])]\n",
    "\n",
    "\n",
    "    #This definition returns the first and last non zero values in a rotated image\n",
    "    #used to calculate and calc the distance between them\n",
    "    def get_nonzero_segments(rotated_image):\n",
    "\n",
    "        # Get the indices of all non-zero elements in the array\n",
    "        nonzero_indices = np.nonzero(rotated_image)[0]\n",
    "\n",
    "        # If there are at least 6 non-zero elements\n",
    "        if len(nonzero_indices) >= 6:\n",
    "            # Extract the first non-zero index\n",
    "            first_nonzero = nonzero_indices[:1]\n",
    "            # Extract the last non-zero index\n",
    "            last_nonzero = nonzero_indices[-1:]\n",
    "        else:  # Cases with fewer than 6 non-zero elements\n",
    "            # Extract all non-zero indices as the first segment\n",
    "            first_nonzero = nonzero_indices[:len(nonzero_indices) // 1]\n",
    "            # Extract all non-zero indices as the last segment\n",
    "            last_nonzero = nonzero_indices[len(nonzero_indices) // 1:]\n",
    "        return first_nonzero, last_nonzero\n",
    "\n",
    "    #This definition returns the first and last non zero values in an image \n",
    "    #used to calculate and calc the distance between them\n",
    "    def get_nonzero_segments1(edge_array):\n",
    "\n",
    "        # Get the indices of all non-zero elements in the array\n",
    "        nonzero_indices = np.nonzero(edge_array)[0]\n",
    "\n",
    "        # If there are at least 6 non-zero elements\n",
    "        if len(nonzero_indices) >= 6:\n",
    "            # Extract the first non-zero index\n",
    "            first_nonzero = nonzero_indices[:1]\n",
    "            # Extract the last non-zero index\n",
    "            last_nonzero = nonzero_indices[-1:]\n",
    "        else:  # Cases with fewer than 6 non-zero elements\n",
    "            # Extract all non-zero indices as the first segment\n",
    "            first_nonzero = nonzero_indices[:len(nonzero_indices) // 1]\n",
    "            # Extract all non-zero indices as the last segment\n",
    "            last_nonzero = nonzero_indices[len(nonzero_indices) // 1:]\n",
    "        return first_nonzero, last_nonzero\n",
    "       \n",
    "    # Find first and last non zero value for each vector     \n",
    "    first_v, last_v = get_nonzero_segments(edges_v)\n",
    "    first_h, last_h = get_nonzero_segments(edges_h) \n",
    "    first_45, last_45 = get_nonzero_segments1(edges_diag_45)\n",
    "    first_neg45, last_neg45 = get_nonzero_segments1(edges_diag_neg45)\n",
    "\n",
    "    #This commmented section is used to display our results and make sense of our features\n",
    "    ######################################################################################\n",
    "    # print(last_v)\n",
    "    # print(first_v)\n",
    "    # print(v_distance)\n",
    "    # plt.plot(first_v, edges_v[first_v],last_v,edges_v[last_v], label=\"Vertical Edges\", color='blue')\n",
    "    # plt.plot(first_h, edges_h[first_h],last_h,edges_h[last_h], label=\"Horizontal  Edges\", color='orange')\n",
    "    # plt.legend(loc=\"upper right\")  # You can adjust loc to place the legend\n",
    "    ######################################################################################\n",
    "\n",
    "    # Find the distance for each vector  \n",
    "    v_distance = last_v - first_v\n",
    "    h_distance = last_h - first_v\n",
    "    distance_45 = last_45 - first_45\n",
    "    distance_neg45 = last_neg45 - first_neg45\n",
    "\n",
    "    # Compute the horizontal/vertical ratio feature\n",
    "    HV_ratio = h_distance*v_distance\n",
    "\n",
    "    #This commmented section is used to monnitor all distanes and make sense of our features\n",
    "    ########################################################################################\n",
    "    # print(last_v)\n",
    "    # print(first_v)\n",
    "    # print(v_distance)\n",
    "    # plt.plot(first_v, edges_v[first_v],last_v,edges_v[last_v], label=\"Vertical Edges\", color='blue')\n",
    "    # plt.plot(first_h, edges_h[first_h],last_h,edges_h[last_h], label=\"Horizontal  Edges\", color='orange')\n",
    "    # plt.legend(loc=\"upper right\")  # You can adjust loc to place the legend\n",
    "    ########################################################################################\n",
    "\n",
    "    # Find the highest vertical and horizontal peaks \n",
    "    peaks_v, _ = find_peaks(edges_v)\n",
    "    peaks_h, _ = find_peaks(edges_h)\n",
    "\n",
    "\n",
    "    # Store the largest vertical and horzitonatl peaks and normalize them\n",
    "    largest_peak_v_idx = peaks_v[np.argmax(edges_v[peaks_v])]  # Index of the largest peak\n",
    "    largest_peak_v_value = (edges_v[largest_peak_v_idx]/255)   # Value of the largest peak\n",
    "    largest_peak_h_idx = peaks_h[np.argmax(edges_h[peaks_h])]  # Correctly use peaks_h\n",
    "    largest_peak_h_value = (edges_h[largest_peak_h_idx]/255)   # Value of the largest peak\n",
    "\n",
    "    # Ensure scalar values for all features\n",
    "    first_v = first_v.item() \n",
    "    v_distance = v_distance.item() \n",
    "    first_h = first_h.item() \n",
    "    h_distance = h_distance.item() \n",
    "    HV_ratio = HV_ratio.item() \n",
    "    first_45 = first_45.item() \n",
    "    distance_45 = distance_45.item() \n",
    "    first_neg45 = first_neg45.item() \n",
    "    distance_neg45 = distance_neg45.item() \n",
    "    \n",
    "    # Create feature vector with the 13 engineerd features and flatten \n",
    "    x = np.array([h_count,edge_density,first_v,v_distance ,first_h, h_distance, HV_ratio,first_45,distance_45 ,first_neg45,distance_neg45,largest_peak_v_value,largest_peak_h_value]).reshape(1, -1)\n",
    "    #print(x)\n",
    "    return x # return the 13 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.917\n",
      "Confusion Matrix:\n",
      " [[21  0  6  0]\n",
      " [ 0 27  0  0]\n",
      " [ 3  0 24  0]\n",
      " [ 0  0  0 27]]\n"
     ]
    }
   ],
   "source": [
    "# This cell loads the training data, trains a Logistic Regression model\n",
    "# to make predictions on the taining set, and evaluates the model's performance by \n",
    "# displaying the accuracy score and a confusion matrix.\n",
    "\n",
    "#Decalring the edge threshold & image dimensions \n",
    "ET = 100\n",
    "width= 64\n",
    "length= 64\n",
    "\n",
    "#Declaring training arrays\n",
    "file_names_train = os.listdir(folder_training)\n",
    "length_train = len(file_names_train)\n",
    "x_train = np.empty((length_train, width)) \n",
    "y_train = np.empty((length_train, 1))\n",
    "\n",
    "# Load the training data\n",
    "x_train, y_train = get_image_data(folder_training, width, length)\n",
    "\n",
    "# Load the 13 engineered features to use to train our model\n",
    "feature_train = np.empty((length_train, 13))\n",
    "for i in range(length_train):\n",
    "    feature_train[i, :] = process_image(x_train[i, :],y_train,ET)\n",
    " \n",
    "\n",
    "#Train the Logistic Regression model \n",
    "model = LogisticRegression(max_iter=5000)\n",
    "model.fit(feature_train, y_train)\n",
    "\n",
    "# Use Sequential Feature Selector to bring down the # of features\n",
    "# Note: this was used when we had more than 13 features\n",
    "# sfs = SequentialFeatureSelector(model, n_features_to_select=13)\n",
    "# sfs.fit(featur_train, y_train)\n",
    "# print(sfs.get_support())\n",
    "\n",
    "#Display Training Accuracy & Confusion Matrix \n",
    "y_train_pred = model.predict(feature_train)\n",
    "print(\"Accuracy:\", np.round(accuracy_score(y_train, y_train_pred),3))\n",
    "print(\"Confusion Matrix:\\n\",confusion_matrix(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.898\n",
      "Confusion Matrix:\n",
      " [[21  0  6  0]\n",
      " [ 0 27  0  0]\n",
      " [ 1  0 23  3]\n",
      " [ 0  0  1 26]]\n"
     ]
    }
   ],
   "source": [
    "# This cell loads the testing data, uses the trained Logistic Regression model \n",
    "# to make predictions on the test set, and evaluates the model's performance by \n",
    "# displaying the accuracy score and a confusion matrix.\n",
    "\n",
    "#Declaring test arrays\n",
    "file_names_test = os.listdir(folder_testing)\n",
    "length_test = len(file_names_test)\n",
    "x_test = np.empty((length_test, width))\n",
    "y_test = np.empty((length_test, 1))\n",
    "\n",
    "# Load the testing data\n",
    "x_test, y_test = get_image_data(folder_testing, width, length)\n",
    "\n",
    "#load the 13 engineered features to use to test our model\n",
    "feature_test = np.empty((length_test, 13))\n",
    "for i in range(length_test):\n",
    "    feature_test[i, :] = process_image(x_test[i, :],y_test[i],ET)\n",
    "\n",
    "# Display Testing Accuracy & Confusion Matrix \n",
    "y_test_pred = model.predict(feature_test)\n",
    "print(\"Accuracy:\", np.round(accuracy_score(y_test, y_test_pred),3))\n",
    "print(\"Confusion Matrix:\\n\",confusion_matrix(y_test, y_test_pred))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
